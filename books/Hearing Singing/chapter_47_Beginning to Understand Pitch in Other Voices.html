<!DOCTYPE html>
<html lang="ru" class="theme-vintage">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Beginning to Understand Pitch in Other Voices - Hearing Singing</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body class="theme-vintage">
    <!-- –ü–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å —Ç–µ–º -->
    <div class="theme-switcher">
        <div class="theme-switcher-label">üé® –¢–µ–º–∞</div>
        <div class="theme-buttons">
            <div class="theme-btn theme-btn-vintage active" onclick="setTheme('vintage')" title="–í–∏–Ω—Ç–∞–∂–Ω–∞—è —Ç–µ–º–∞"></div>
            <div class="theme-btn theme-btn-dark" onclick="setTheme('dark')" title="–ß–µ—Ä–Ω–∞—è —Ç–µ–º–∞"></div>
        </div>
    </div>
    
    <div class="book-header">
        <h1 class="book-title">Hearing Singing</h1>
        <p class="book-author">Ian Howell</p>
    </div>
    
    <div class="navigation">
        <a href="index.html" class="nav-button">üìö –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ</a>
        <a href="chapter_46_Issues with the Singer‚Äôs Formant and Pitch.html" class="nav-button">‚Üê –ü—Ä–µ–¥—ã–¥—É—â–∞—è</a>
        <a href="chapter_48_Conclusions and Potential Applications.html" class="nav-button">–°–ª–µ–¥—É—é—â–∞—è ‚Üí</a>
    </div>
    
    <div class="chapter-content" data-chapter-title="Beginning to Understand Pitch in Other Voices">
        <h3>–°—Ç—Ä–∞–Ω–∏—Ü–∞ 112</h3>

<p>BEGINNING TO UNDERSTAND PITCH IN OTHER VOICES Momentarily setting aside the more thoroughly studied voices of classical basses, baritones, and tenors, let us consider the implications of pitch perception for the study of voices more broadly. In particular, let us consider the study of endogenous estrogen puberty classical singers, countertenors, and contemporary singers regardless of sex or hormonal regime. As we have discussed, most speech research historically targeted formants between 300 Hz and 3.4 kHz. As such, many spectrographs default to a frequency display range that shows no information above 4 to 5 kHz. We recall that this condensed range makes sense in the historical context of the analogue telephone bandwidth and the study of speech and language. But the spectra of many ways of singing routinely exceed the upper frequency range of speech. Consider the same soprano from figure 5.1 singing an A ‚ô≠ 4 (figure 5.6, above) and an A ‚ô≠ 5 (figure 5.6, below). When the spectrograph caps the visible frequency range at 5 kHz, up to twelve harmonics are visible for the pitch A ‚ô≠ 4. However, this truncation leaves only six harmonics visible for the A ‚ô≠ 5. This suggests that no unresolved harmonics exist in this voice. At minimum, one would need to know to look for them in the frequency information above this graph. This should push us to ask whether the hidden frequency information is perceptually relevant.</p>

<hr class="page-separator">

<h3>–°—Ç—Ä–∞–Ω–∏—Ü–∞ 113</h3>

<p>Figure 5.6 The soprano from Figure 5.1 singing an A ‚ô≠ 4 (above) and A ‚ô≠ 5 (below). Note that when the frequency range (horizontal axis) is capped at 5 kHz, only six harmonics are visible at the higher pitch. Source: Recorded by author under controlled conditions. I must reiterate that the portion of the spectrum shown as unresolved harmonics will contribute something perceptually different than the portion shown as resolved harmonics. This is a qualitative difference relevant to the sound of a singing voice. In some singers there will be little spectral energy above 5 kHz. Consider the edge case of a low- intensity, speech-pitch range lullaby sung to a child on the verge of falling asleep. Most likely there will be no high-frequency energy; it almost certainly will not significantly impact language cognition even if it were perceptible. Even so, the artificial visual limit constrains the imagination. I</p>

<hr class="page-separator">

<h3>–°—Ç—Ä–∞–Ω–∏—Ü–∞ 114</h3>

<p>would go so far as to suggest that it inevitably limits the questions one may be likely to ask about how to listen to these voices. By extension, a spectrum capped at 4 to 5 kHz implies that a voice singing above the treble staff will obligatorily elicit only a resolved sound. We know that is not necessarily so. It further suggests that the broad perceptual quality of unresolved brightness is only available to lower voices. The converse is also deeply problematic: The SFC model may push singers to imagine that sub-SFC energy may make no meaningful contribution to the sound in classical bass, baritone, and tenor voices. Additionally, the harmonics shown above the fundamental in figure 5.6 (bottom) contribute to both the timbre and the pitch percept. One cannot assume that the timbral quality of a treble voice above the treble staff is fully captured in the sound of its fundamental, despite its obvious power. It is also worth pointing out that by comparison to their unamplified counterparts, singers who use electronic amplification may well capture sounds with significantly more of this high-frequency energy. The voice pedagogy and vocology communities should grapple with the question of whether to disregard such high-frequency energy. Linguists suggest that we have captured enough of the spectrum in figure 5.6 to preserve language cognition. But that view excludes aspects of the sound that may be integral to the aesthetics of the singing voice. If the focus of our study were a violin, we wouldn‚Äôt think to constrain spectrum in this manner, or to suggest that such a limited image reasonably captures the essence of the violin‚Äôs character. In fact, raising the upper frequency limit of this image to 12 kHz reveals an additional, audible portion of the spectrum previously excluded (see figure 5.7). It is fair to debate the perceptual relevance of this higher frequency energy, but we have already learned that we can predict this spectral activity to contribute pitch-less noise. We cannot discuss that of which we are unaware.</p>

<hr class="page-separator">

<h3>–°—Ç—Ä–∞–Ω–∏—Ü–∞ 115</h3>

<p>Figure 5.7 The soprano from Figure 5.6 singing an A ‚ô≠ 5. Note that an additional spectral peak is revealed when the frequency range (horizontal axis) is extended to 12 kHz. Source: Recorded by author under controlled conditions. Two additional examples highlight the distance between how we might model the acoustics of a voice in a way that excludes high-frequency energy and what the actual experience of that voice is. In ‚ÄúWhat Was I Made For?‚Äù from the soundtrack to the 2023 film Barbie, Billie Eilish sings with a distinctively breathy sound. Listen to the excerpted spectrum from the lyric ‚ÄúI‚Äù found around 0:11 in the recording (see figure 5.8). Four harmonics appear below 2.5 kHz, indicating periodic, pitched aspects of her sound. The peaks centered at 3.5 kHz and 10 kHz are dominated by breathy turbulence and convey little to no pitch information related to the fundamental. I would suggest that we label this higher frequency energy as stochastic and unorganized. It adds qualitative, bright aspects to the overall timbre but does not impact vowel or pitch identification. Worth considering as well is the fact that high-frequency hearing loss is a natural consequence of aging.21 While I can clearly hear the qualitative aspects of her voice up to around 12 kHz, I cannot perceive her energy above that. When I called my ten-year-old daughter into the room, she was able to clearly hear higher peaks. Researchers using their own senses to contemplate the timbre of a voice may inadvertently impose their own hearing limitations on the descriptive models they create. This also has</p>

<hr class="page-separator">

<h3>–°—Ç—Ä–∞–Ω–∏—Ü–∞ 116</h3>

<p>implications for audio signal chain considerations in perceptual studies, and for our understanding of the auditory perception of those who wear hearing aids. Figure 5.8 Spectrum (left) and spectrogram (right) of Billie Eilish singing ‚ÄúI‚Äù from ‚ÄúWhat Was I Made For?.‚Äù Source: https://www.youtube.com/watch?v=cW8VLC9nnTo. Let us consider contrasting examples from James Brown‚Äôs 1965 single ‚ÄúI Got You (I Feel Good).‚Äù Mr. Brown‚Äôs initial fry scream with distortion, loosely transcribed as [w√¶u], contains little to no periodic oscillation of the vocal folds at all (see figure 5.9). Yet during the sustained [√¶], we experience the chaotically excited, ongoing first resonance of the vocal tract (around 1,080 Hz) as the closest thing to a pitch. The higher frequency, equally noisy resonance peaks contribute nothing to the pitch percept. Instead, they add important qualitative elements to the overall timbre. A few seconds later, he sings ‚Äúfeel‚Äù (see figure 5.10) with reasonably clear harmonics in the spectrum up to 10 kHz, which is remarkable considering the age of the recording. I would argue that the now better-organized information centered at 7 kHz contributes a pitch- less, buzzy quality that distinguishes Mr. Brown‚Äôs sample from the turbulent noise in Ms. Eilish‚Äôs sample.</p>

<hr class="page-separator">

<h3>–°—Ç—Ä–∞–Ω–∏—Ü–∞ 117</h3>

<p>Figure 5.9 Spectrum (left) and spectrogram (right) of James Brown‚Äôs fry scream with distortion [w√¶u] from ‚ÄúI Got You (I Feel Good).‚Äù Aproximate pitch percept at ~1,080 Hz indicated with a dotted line. Source: https://www.youtube.com/watch?v=W-rn7i_ETYc. Figure 5.10 Spectrum (left) and spectrogram (right) of James Brown‚Äôs singing ‚Äúfeel‚Äù from ‚ÄúI Got You (I Feel Good).‚Äù Source: https://www.youtube.com/watch?v=W-rn7i_ETYc. Some may critique my use of commercially recorded singers here. To be sure, the recording process has a mediating effect on the sound of a singer. But consider that we have long understood the electronic signal chain to be an integral extension of modern musical instruments. Were we to study the sound of an electric guitar, we would not require the player to remove the amplification or associated electronics they use to shape</p>

<hr class="page-separator">

<h3>–°—Ç—Ä–∞–Ω–∏—Ü–∞ 118</h3>

<p>their sound. One does not drill down to the essence of Jimi Hendrix‚Äôs guitar playing by removing the distortion his amplifier provided. Artists like Hendrix crafted their sound to include the transformation of the original acoustic information by these electronics. Removing them destroys the ecological validity of such a study. In the case of Ms. Eilish‚Äôs recording, she certainly benefits from her intimate proximity to the microphone and the electronic processes that help to balance and amplify the breathy and pitched aspects of her singing. This should be an a priori assumption for those of us who study unamplified classical singers as well. It is considered the best practice to control the acoustical variables inherent in a concert hall by recording singers with laboratory equipment in a controlled setting. But does this not similarly risk harming ecological validity in potentially unsolvable ways? Perhaps we can append Kai Siedenburg and Stephen McAdams‚Äôs statement that ‚Äúthere does not exist the bassoon timbre, but rather a bassoon timbre at a given pitch and dynamic‚Äù22 in a specific space at a specific distance with or without an electronic signal chain for a listener of a specific age and hearing acuity. Remember that radiation is as important a subsystem of the voice as perception. When our well-intentioned lab setups control variables in ways that destroy the timbral qualities inherent to a genre, we must pause to reflect whether we are measuring that genre anymore. At best this muddies the way forward, but we should not discount aspects of a singer‚Äôs sound just because they were consciously facilitated by external means. As we move forward to chapters that focus on auditory roughness, tone color, and the ways in which these time-invariant aspects of timbre interact in predictable patterns to explain familiar phenomena, I will similarly push to reform some widely accepted models to better represent the way in which humans actually perceive the sound of a singer.</p>

<hr class="page-separator">
    </div>
    
    <div class="navigation">
        <a href="index.html" class="nav-button">üìö –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ</a>
        <a href="chapter_46_Issues with the Singer‚Äôs Formant and Pitch.html" class="nav-button">‚Üê –ü—Ä–µ–¥—ã–¥—É—â–∞—è</a>
        <a href="chapter_48_Conclusions and Potential Applications.html" class="nav-button">–°–ª–µ–¥—É—é—â–∞—è ‚Üí</a>
    </div>
    
    <script src="/static/js/notes.js"></script>
    <script>
        // –§—É–Ω–∫—Ü–∏—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ç–µ–º
        function setTheme(theme) {
            document.body.className = document.body.className.replace(/theme-\w+/g, '');
            document.documentElement.className = document.documentElement.className.replace(/theme-\w+/g, '');
            
            document.body.classList.add('theme-' + theme);
            document.documentElement.classList.add('theme-' + theme);
            
            localStorage.setItem('reading-theme', theme);
            
            document.querySelectorAll('.theme-btn').forEach(btn => btn.classList.remove('active'));
            document.querySelector('.theme-btn-' + theme).classList.add('active');
        }
        
        function loadSavedTheme() {
            const savedTheme = localStorage.getItem('reading-theme') || 'vintage';
            setTheme(savedTheme);
        }
        
        document.addEventListener('DOMContentLoaded', function() {
            loadSavedTheme();
            
            const pathParts = window.location.pathname.split('/');
            if (pathParts[1] === 'book' && pathParts[2]) {
                const bookPath = pathParts[2];
                fetch(`/api/book-info?path=${encodeURIComponent(bookPath)}`)
                    .then(response => response.json())
                    .then(data => {
                        if (data.book_id && window.notesSystem) {
                            window.notesSystem.bookId = data.book_id;
                            window.notesSystem.addNotesButtonToNavigation();
                        }
                    })
                    .catch(error => console.log('Could not load book info:', error));
            }
        });
    </script>
</body>
</html>